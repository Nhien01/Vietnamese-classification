{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "# Chia tập train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "test_percent = 0.2\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    " \n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    " \n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "        else:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        \n",
    "    return ''.join(chars)\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thống kê số lượng data theo nhãn\n",
    "count = {}\n",
    "for line in open('file_text.txt', encoding='utf-8'):\n",
    "    key = line.split()[0]\n",
    "    count[key] = count.get(key, 0) + 1\n",
    "\n",
    "# for key in count:\n",
    "#     print(key, count[key])\n",
    "  \n",
    "total_label = 11\n",
    "vocab = {}\n",
    "label_vocab = {}\n",
    "for line in open('file_text.txt', encoding='utf-8'):\n",
    "    words = line.split()\n",
    "    label = words[0]\n",
    "    if label not in label_vocab:\n",
    "        label_vocab[label] = {}\n",
    "    for word in words[1:]:\n",
    "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
    "        if word not in vocab:\n",
    "            vocab[word] = set()\n",
    "        vocab[word].add(label)\n",
    "\n",
    "count = {}\n",
    "for word in vocab:\n",
    "    if len(vocab[word]) == total_label:\n",
    "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
    "        \n",
    "sorted_count = sorted(count, key=count.get, reverse=True)\n",
    "\n",
    "stopword = set()\n",
    "# with open('stopwords.txt', 'r', encoding='utf-8') as fp:\n",
    "for word in sorted_count[:100]:\n",
    "    stopword.add(word)\n",
    "        # fp.write(word + '\\n')\n",
    "    \n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for line in open('file_text.prep', encoding='utf-8'):\n",
    "    words = line.strip().split()\n",
    "    label.append(words[0])\n",
    "    text.append(' '.join(words[1:]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__du_lịch', '__label__giáo_dục', '__label__giải_trí', '__label__phim', '__label__sức_khỏe', '__label__thế_giới', '__label__thể_thao', '__label__thời_sự', '__label__thời_trang', '__label__xe', '__label__âm_nhạc'] \n",
      "\n",
      "Hoa_hậu Phương_Khánh thay_đổi mái_tóc dài gắn_bó lâu sang kiểu ngắn , chuộng gu mặc tối_giản , thanh_lịch tuổi 30 . Hôm 26/4 , đẹp tung bộ ảnh . Nhiều khán_giả nhận_xét diện_mạo Phương_Khánh trông khỏe khoắn , cuốn_hút . Trước_đây , hoa_hậu luôn gắn_bó mái_tóc dài ngang lưng . Khi dự sự_kiện đời_thường , diện váy áo phong_cách nhẹ_nhàng . Khi cắt tóc ngắn , mặc đa_dạng . Người đẹp_phối trang_phục phụ_kiện gồm mắt_kính , túi_xách , trang_điểm , kiểu tóc tự_nhiên . Cô chuộng gam đơn_sắc , phần_lớn màu trung_tính . Ngoài phong_cách , đẹp chú_trọng chăm_sóc da nhằm giữ vẻ tươi_trẻ . Cô thường áp_dụng phương_pháp xông_hơi da mặt tinh_dầu , đắp mặt_nạ trái_cây , spa rảnh tẩy tế_bào chết , ngăn_ngừa lão hóa . Hoa_hậu rèn_luyện vóc_dáng tập_yoga , gym . Cô chạy bộ , tập_tạ huấn_luyện_viên tăng sức_mạnh phần cơ_tay , chân . Cô chế_độ kiêng tinh_bột chất ngọt , bổ_sung trái_cây , ép . Trên trang cá_nhân hoa_hậu , khán_giả nhận_xét toát vẻ thanh_lịch diện đầm_lụa phong_cách cổ_điển , phối_túi Louis_Vuitton . Phương_Khánh_sinh 1994 Bến_Tre , 1,71 m . Trong chung_kết Miss_Earth 11/2018 , vượt 87 thí_sinh đăng_quang . Sau cuộc thi , chuyên_trang sắc_đẹp đánh_giá . Cô lọt top 10 \" Timeless_beauty \" Missosology_bình chọn ( H'Hen_Niê đứng ) . Người đẹp tiếp_tục theo_đuổi hoạt_động cộng_đồng , thường thực_hiện video cá_nhân chia_sẻ lối sống khỏe mạnh , hướng tự_nhiên . 8 \n",
      "\n",
      "HÀ NỘIThua_Thể Công_0-2 tối_9/5 , Hà_Nội FC_kém bảng Nam_Định 15 điểm 17 vòng V-League 2023 - 2024 . Hà_Nội FC_đứng thứ_bảy 23 điểm , bảng Nam_Định 38 điểm giải đấu chín vòng . Với tình_thế hiện_tại , ngay vị_trí top thách_thức đối_với đội bóng thủ_đô . Hiện , kém đội đứng thứ ba Bình_Định điểm . Trên sân nhà_Hàng_Đẫy , Hà_Nội FC_nhập cuộc tốt , tiền đạo Phạm_Tuấn_Hải bỏ lỡ cơ_hội ngon ăn_ở phút thứ 9 . Từ cánh , bóng_chuyền trung_lộ chân_Văn_Quyết rồi Denilson_Pereira vòng cấm Tuấn_Hải . Tiền_đạo sinh 1998 giả_sút ngoặt bóng thủ_môn hậu_vệ Thể_Công_lỡ trớn , đưa bóng chệch khung_thành rộng_mở . Đến phút 14 , Thể_Công_đáp lễ cú đánh_đầu dội cột dọc Jaha quả đá phạt_góc cánh trái . Bảy_phút , Thể_Công_phản_công cánh rồi chuyền trung_lộ Nhâm_Mạnh_Dũng chọc_khe vòng cấm . Nguyễn_Hoàng_Đức di_chuyển thông_minh thoát khỏi kèm_cặp Tim_Hall , rồi cứa_lòng chân trái mở tỷ_số . Đây bàn thứ Hoàng_Đức V-League 2023 - 2024 , bàn đầu_tiên 2024 . Bàn tiền vệ_sinh 1998 vòng 7 trận thua Bình_Định 1-4 . Ở phút bù , thủ_môn Quan_Văn_Chuẩn lao tham_gia tấn_công tình_huống cố_định bất_thành . Đến phút bù thứ_bảy , hàng_thủ Hà_Nội mắc sai_lầm Lê_Văn_Xuân đánh_đầu chuyền thủ_môn quá nhẹ , tạo điều_kiện Trương_Tiến_Anh dứt_điểm xâu kim_Văn_Chuẩn ấn_định tỷ_số 2-0 . Ba_điểm quan_trọng giúp Thể_Công 20 điểm đứng thứ 12 , Hà_Tĩnh xếp kém_hiệu số bàn thắng_bại - 7 so - 6 . Thể_Công_nới rộng khoảng_cách vị_trí cuối bảng Khánh_Hòa vị_trí đá play-off SLNA 10 điểm . Đội_hình xuất_phát Hà_Nội FC : Quan_Văn_Chuẩn , Đỗ_Duy_Mạnh , Thành_Chung , Phạm_Xuân_Mạnh , Vũ_Đình_Hai , Tim_Hall , Đậu_Văn_Toàn , Đỗ_Hùng_Dũng , Phạm_Tuấn_Hải , Văn_Quyết , Denilson Pereira_Thể_Công : Quàng_Thế_Tài , Thanh_Bình , Bùi_Tiến_Dũng , Đặng_Tuấn_Phong , Trương_Tiến_Anh , Đức_Chiến , Hoàng_Đức , Khuất_Văn_Khang , Jaha , Nhâm_Mạnh_Dũng , Joao_Pedro . 6\n"
     ]
    }
   ],
   "source": [
    "# encode label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print(list(label_encoder.classes_), '\\n')\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(X_train[0], y_train[0], '\\n')\n",
    "print(X_test[0], y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Accuracy = 0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Naive Bayes\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   __label__du_lịch       1.00      0.10      0.18        20\n",
      "  __label__giáo_dục       1.00      0.93      0.96        14\n",
      "  __label__giải_trí       1.00      0.13      0.24        15\n",
      "      __label__phim       0.93      0.81      0.87        16\n",
      "  __label__sức_khỏe       1.00      0.67      0.80        24\n",
      "  __label__thế_giới       0.93      0.54      0.68        26\n",
      "  __label__thể_thao       0.91      0.98      0.94        50\n",
      "   __label__thời_sự       0.38      1.00      0.55        34\n",
      "__label__thời_trang       1.00      0.45      0.62        11\n",
      "        __label__xe       1.00      0.28      0.43        18\n",
      "   __label__âm_nhạc       0.62      0.92      0.74        25\n",
      "\n",
      "           accuracy                           0.70       253\n",
      "          macro avg       0.89      0.62      0.64       253\n",
      "       weighted avg       0.85      0.70      0.67       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
    "y_pred = nb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [6]\n",
      "Predict label: ['__label__thể_thao']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document = \"hôm nào chúng ta cùng nhau chơi bóng rổ nhé\"\n",
    "\n",
    "document = text_preprocess(document)\n",
    "document = remove_stopwords(document)\n",
    "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
    "label = nb_model.predict([document])\n",
    "print(\"Label:\", label)\n",
    "print('Predict label:', label_encoder.inverse_transform(label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
